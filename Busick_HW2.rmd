---
title: "HW 2"
author: "Hunter Busick"
date: "9/25/2024"
output: 
  html_document:
    number_sections: true
---

This homework is meant to illustrate the methods of classification algorithms as well as their potential pitfalls.  In class, we demonstrated K-Nearest-Neighbors using the `iris` dataset.  Today I will give you a different subset of this same data, and you will train a KNN classifier.  

```{r, echo = FALSE}
set.seed(123)
library(class)

df <- data(iris) 

normal <-function(x) {
  (x -min(x))/(max(x)-min(x))   
}

iris_norm <- as.data.frame(lapply(iris[,c(1,2,3,4)], normal))

subset <- c(1:45, 58, 60:70, 82, 94, 110:150)
iris_train <- iris_norm[subset,] 
iris_test <- iris_norm[-subset,] 

iris_target_category <- iris[subset,5]
iris_test_category <- iris[-subset,5]

```

#
Above, I have given you a training-testing partition.  Train the KNN with $K = 5$ on the training data and use this to classify the 50 test observations.  Once you have classified the test observations, create a contingency table -- like we did in class -- to evaluate which observations your algorithm is misclassifying.   

```{r}
set.seed(123)

kneighb <- knn(iris_train, iris_test, cl = iris_target_category, k = 5)
kntab <- table(kneighb, iris_test_category)
kntab
accuracy <- function(x){
  sum(diag(x)/(sum(rowSums(x)))) * 100
}
accuracy(kntab)
```

#

Discuss your results.  If you have done this correctly, you should have a classification error rate that is roughly 20% higher than what we observed in class.  Why is this the case? In particular run a summary of the `iris_test_category` as well as `iris_target_category` and discuss how this plays a role in your answer.  


```{r}
summary(iris_test_category)
summary(iris_target_category)
```
Running a quick summary of our *iris_test_category* and *iris_target_category* gives us good insight into why our classification error rate has jumped compared to the analysis performed in class. Because we have sampled our training and testing partitions in a nonrandom fashion, we are subject to improper classifications as a result of the structure of our data. Our data is structured such that the first 50 observations are setosa, next 50 versicolor, and final 50 virginica. The *subset* variable partitions the data so that we train the KNN on 86% setosa and virginica flowers, and only 14% versicolor, as outlined by our summaries for the target and test categories. This explains why the KNN improperly labeled 11 versicolor plants as virginica, as the training set hardly contained versicolor flowers.

#

Choice of $K$ can also influence this classifier.  Why would choosing $K = 6$ not be advisable for this data? 

We would not want to choose $K = 6$ when performing a KNN on this data because we have three classifications. Having $K = 6$ opens the door to having a three way tie for number of nearest neighbors where there are two of each classification. This also opens the door for a scenario where there is a tie between two classifications, each having 3 neighbors. Ideally we want to avoid using K's that are divisible by the number of classifications to avoid potential ties and coin-tosses.

#

Build a github repository to store your homework assignments.  Share the link in this file.  

https://github.com/hbusick/HW2.git

